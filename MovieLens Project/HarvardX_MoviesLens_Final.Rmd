---
title: "HarvardX Capstone MovieLens Project"
author: "Raphael Oliveira Abreu"
date: "2023-02-05"
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Over the last decade, data analysis and data science have experienced explosive growth and have become integral parts of many industries, including business, finance, healthcare, social sciences, and many others. Data science has also become a key driver of innovation and business growth, with companies using data-driven insights to make informed decisions and drive revenue. As the amount of data continues to grow, the field of data analysis and data science is expected to become even more important in the years to come.

Before gaining popularity it has today, in 2006, Netflix opened a competition called Netflix Prize for the best collaborative filtering algorithm to predict user rating based on films. This capstone project shares objectives with the Neflix Prize. The goal of this project is to explore, develop and train a machine learning model recommendation system based on the [MovieLens 10M dataset](https://grouplens.org/datasets/movielens/10m/), a data set containing 10 millions ratings applied for 10 thousand movies by 72 thousand users. The goal of this project is to predict ratings with a root mean square error (RMSE) of less than 0.86490.

First, it will be conducted an exploratory data analysis of the validation set utilizing data manipulation and common visualization approaches. Then, it will be developed, trained and tested a recommendation system algorithm. At the end, the results will be shared followed by recommendations to be taken into account in the future.


# Analysis

The sets edx and final_holdout_test used in this project where created based on code provided by the course instructions.
Here are the results of the data exploration conducted on the edx data set.

```{r sets codes, echo  = FALSE, include = FALSE}
##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

##########################################################

```

```{r analysis intro, echo = FALSE}
edx %>% head() %>% knitr::kable(position = "left")
```


```{r anaysis intro 2, echo = FALSE, include = FALSE}
edx %>% class()
edx %>% glimpse()
```
The edx set has is a data.frame type object with 9,000,055 rows and 6 columns: userId, movieId, rating, timestamp, title and genres.


## Ratings

```{r ratings, echo = FALSE, include = FALSE}
length(edx$rating)
mean(edx$rating)
length(unique(edx$rating))
sort(unique(edx$rating))
```
The edx set has a total of 9,000,055 ratings by users. These ratings have 10 possible values, ranging from 0.5 to 5 in increments of 0.5. The distribution of the ratings can be see in the figure below. Also worth noting that the average rating of the whole set is approximate 3.51.


```{r ratings distribution, echo = FALSE, message = FALSE}
options(scipen=5)
edx %>% ggplot(aes(rating)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(0, 5, 0.5)) +
  xlab("Rating") +
  ylab("Number of ratings") +
  ggtitle("Number of ratings by rating grade")
```


## Movies

```{r, echo = TRUE, include = FALSE, message= FALSE}
length(unique(edx$movieId))
edx %>%
  group_by(title) %>%
  summarise(n_rating = n()) %>%
  arrange(desc(n_rating))
edx %>%
  group_by(movieId) %>%
  summarise(n_rating = n()) %>%
  filter(n_rating == 1) %>%
  count()
```
The set has 10677 unique movies which are cataloged with 20 genres classifications available. The movie with the most ratings is Pulp Fiction with 31362 ratings. The bottom of the list is composed of 126 movies receiving only one rating.
When we make a distribution of the movies by average rating the shape is very similar to users by average rating.

```{r Number of movies by number of ratings, echo = FALSE, message = FALSE}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) +
  geom_histogram() +
  scale_x_continuous(trans="log10") +
  xlab("Number of movies") +
  ylab("Number of ratings") +
  ggtitle("Number of movies by number of ratings")
```

```{r number of movies by average rating, echo = FALSE, message = FALSE}
edx %>%
  group_by(movieId) %>%
  summarise(avg_rating = mean(rating)) %>%
  ggplot(aes(avg_rating)) +
  geom_histogram() +
  xlab("Average rating") +
  ylab("Number of movies") +
  ggtitle("Number of movies by average rating")
```


## Users

By exploring the userId variable the minimum number of movies a user reviewed is 10 times by one single user, and the maximum is 6616. 3470 users reviewed as few as 20 movies. It is also safely to say that no user reviewed the same movie twice since there is no duplication between the first and second variables, userId and movieId.
Distributing the users of the dataset by the average rating each user have we can come upon a bell shaped figure.

```{r users, echo = FALSE, include = FALSE, message = FALSE}
length(unique(edx$userId))
edx[duplicated(edx[,1:2]),]
sum(duplicated(edx[,1:2]))
edx_l <- edx %>%
  group_by(userId) %>%
  summarize(n_rating = n())
sum(edx_l$n_rating)/nrow(edx_l)
edx %>%
  group_by(userId) %>%
  summarize(n_rating = n()) %>%
  arrange(desc(n_rating))
edx %>%
  group_by(userId) %>%
  summarize(n_rating = n()) %>%
  arrange(n_rating) %>%
  filter(n_rating < 20)
```


```{r users number of ratings by number of users, echo = FALSE, message = FALSE}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) +
  geom_histogram() +
  scale_x_continuous(trans="log10") +
  xlab("Number of users") +
  ylab("Number of ratings") +
  ggtitle("Number of user by number of ratings")
```
  

```{r number of users by average rating, echo = FALSE, message = FALSE}
edx %>%
  group_by(userId) %>%
  summarise(avg_rating = mean(rating)) %>%
  ggplot(aes(avg_rating)) +
  geom_histogram() +
  xlab("Average rating") +
  ylab("Number of users") +
  ggtitle("Number of user by average rating")
```


## Genres

```{r genre, echo = FALSE, include = FALSE}
length(unique(unlist(strsplit(edx$genre, split = "\\|"))))
unique(unlist(strsplit(edx$genre, split = "\\|")))
```
As said before, the data set utilizes 20 available genre types for the classification of the movies. Below, we see how many times each type was used for classification and the average that genre got across the full edx dataset.

```{r ratings distrubution table, echo = FALSE}
if(!require(tidyr)) install.packages("tidyr")
library(tidyr)
edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarise(count = n(), rating = round(mean(rating), 3)) %>%
  arrange(desc(count)) %>% 
  knitr::kable()
```


## Title (and release year)

The title variable contains the title of the movie with the addition of the year of release between parenthesis. A mutated dataset (edx_m) is build in order to extract and explore if the year of release has any influence in the ratings, and still keep the original edx dataset.

```{r extracting the yera of release from title, include = FALSE}
edx_m <- edx %>% #edx_m for edx mutated
  mutate(release_year = as.numeric(str_sub(title, -5, -2)))
```

90's movies appears as the most reviewed release years followed by 2000's movies. This makes sense since these are the eras are contemporary to the in which the ratings where made. 80's movies follows in third followed by its preceding decade. This trend can be seen thought the rest of the release years.

```{r tittle of the top 20 release year by number of ratings, echo = FALSE, include = FALSE}
edx_m %>% group_by(release_year) %>%
  summarise(number_of_ratings = n()) %>%
  arrange(desc(number_of_ratings)) %>%
  head(20) 
```


```{r line plot of number of ratings by release year, echo = FALSE}
edx_m %>% group_by(release_year) %>%
  summarise(number_of_ratings = n()) %>%
  ggplot(aes(release_year, number_of_ratings)) +
  geom_line() +
  xlab("Release Year") +
  ylab("Number of ratings") +
  ggtitle("Number of ratings by Release year")
```


## Timestamp

The timestamp column records the time the rating review was made, but it records in an specific way. The timestamp is the number of seconds that have elapsed since midnight January 1, 1970. So before continuing the analysis, it was necessary to transform the variable into a date format. The timestamp was also mutated into two new columns: one agglutinating dates into weeks, since the visualization of day to day data would be extremely crowded, and the second regarding the four seasons, which was made with the assistance of the [hydroTSM](https://cran.r-project.org/web/packages/hydroTSM/index.html) package.

```{r converting and mutanting timestamp, echo = FALSE, include = FALSE}
if(!require(hydroTSM)) install.packages("hydroTSM", repos = "http://cran.us.r-project.org")
if(!require(libridate)) install.packages("libridate", repos = "http://cran.us.r-project.org")
library(hydroTSM)
library(lubridate)
edx_m <- edx_m %>% 
  mutate(rating_date = as.Date(as.POSIXct(timestamp, origin = "1970-01-01")), 
         rating_week = round_date(as_datetime(timestamp), unit = "week"), 
         season = time2season(rating_date, out.fmt = "seasons"))
edx_m$season <- as.factor(edx_m$season)
class(edx_m$season)
levels(edx_m$season) #autumn is wrtitten wrong
levels(edx_m$season) <- c("autumn", "spring", "summer", "winter")
table(edx_m$season)
```

```{r exploring timestamp, echo = FALSE, include = FALSE}
range(edx_m$rating_week)
table(edx_m$rating_week)
length(unique(edx_m$rating_week))
```
The dataset rating dates spams though almost 15 year, from 1995 to 2009. 1995 is the year with the least reviews, only 2. While 2000 is the year with the most reviews.

```{r bar plot rating year x number of ratings, echo = FALSE}
edx_m %>%
  ggplot(aes(rating_week)) +
  geom_bar() +
  xlab("Rating Week") +
  ylab("Number of ratings") +
  ggtitle("Number of ratings by rating week")
```
At first, there is a downward trend in average ranting throughout the first years that becomes stable from 1998 forward. The downward trend is more accentuated when dates are grouped by year.

```{r scatter plot rating year x average rating, echo = FALSE, message = FALSE}
edx_m %>%
  group_by(rating_week) %>%
  summarise(avg_rating = mean(rating)) %>%
  ggplot(aes(rating_week, avg_rating)) +
  geom_point() +
  geom_smooth() +
  xlab("Rating Week") +
  ylab("Average rating") +
  ggtitle("Number of ratings by rating week")
```

Analysing timestamp as seasons of the year no trend can be seen, since all seasons have approximate the same number of reviews, although autumn has slightly more ratings than the average and spring is the season with the least number of reviews. When analyzing the average rating of each season no significant differences can be found between the seasons. All season rating are in a 0.05 interval.

```{r column graph rating season x number of ratings, echo = FALSE}
edx_m %>%
  ggplot(aes(season)) +
  geom_bar() +
  xlab("Season") +
  ylab("Number of ratings") +
  ggtitle("Number of ratings by season")
```

```{r scatter plot rating season x average rating, echo = FALSE, message = FALSE}
edx_m %>%
  group_by(season) %>%
  summarise(avg_rating = mean(rating)) %>%
  ggplot(aes(season, avg_rating)) +
  geom_point() +
  geom_smooth() +
  xlab("Season") +
  ylab("Average rating") +
  ggtitle("Average rating by season")
```

# Method and Results


## RMSE

The RMSE measures the average distance between the predicted ratings and the actual ratings in the test set. We can interpret it as the typical error we make when predicting a movie rating. It is a commonly used metric to evaluate the performance of regression models, including those used for predicting movie ratings. The lower the RMSE, the better the model is at predicting ratings.

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$

where:

* $N$ is the total number of user/movie combinations.
* $\hat{y}_{u,i}$ is the predicted rating for the same movie and user.
* $y_{u,i}$ is the actual rating provided by user $u$ for movie $u$.

The project aimed to create an algorithm that achieved an RMSE value below 0.86490, as specified in the project objectives.


## Algorith

Before any calculations the splitting of the edx data set must occur in order to save the final_holdout set and still have a train set and a test set. The set will be splitted with the proportions 80% for the train set and 20% for the test set.

```{r splitting edx, echo = FALSE}
set.seed(42)
test_index <- createDataPartition(y = edx_m$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx_m[-test_index,]
test_set <- edx_m[test_index,]
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```


### Naive RMSE

The simplest prediction model is predicting the same rating for all movies regardless of user $u$ and movie $i$ meaning that the final result being the same is simply a random coincidence. This model can the written like this:

$${Y}_{u,i} = \mu + \epsilon_{u,i}$$

Where:

* ${Y}_{u,i}$ is the actual rating for movie $i$ by user $u$
* $\mu$ represents the sum of all the movies $i$ and users $u$
* $\epsilon_{u,i}$ is the independent erros sampled from the same distribution centered at zero.

The average of all ratings is the estimate of $\mu$ that minimizes the RMSE. Thus, $\hat{mu}$ as the mean of the ratings of the train set is the simple formula used to train the first algorithm.

```{r naive_RMSE table, echo = FALSE, message = FALSE}
mu <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu)
rmse_goal <- 0.86490
rmse_results <- data.frame(method = "Project objective", RMSE = "0.86490")
rmse_results <- rbind(rmse_results,
                      data_frame(method ="Simple Average", 
                                 RMSE = round(naive_rmse, 5)))
rmse_results %>% knitr::kable()
```
With a RMSE higher than 1 we can say that the predicted ratings is 1 star away from the actual rating.


### Adding the movie factor

The previous section revealed that ratings were not evenly distributed among all movies. Some movies received higher average ratings than others, and taking this bias into account would enhance the prediction accuracy. Here, this effect can be translated into $b_i$.

$$Y_{u,i} = \mu + b_i + \epsilon_{u,i}$$  

```{r movie_RMSE, echo = FALSE}
movies_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```

```{r movie_RMSE table, echo = FALSE}
predicted_b_i <- mu + test_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred
plus_movie_rmse <- RMSE(predicted_b_i, test_set$rating)
rmse_results <- rbind(rmse_results, 
                          data.frame(method = "Movie Effect Model", 
                                     RMSE = round(plus_movie_rmse, 5)))
rmse_results %>% knitr::kable()
```


### Adding the user factor

The previous section also revealed that ratings were not evenly distributed among all users and the majority ranging in middle. Some could also have given a bad rating for a good movie and vice-versa. Consequently, the training algorithm can be fine-tuned with the addition of the factor $b_u$ for users into the model, which behavior is similar to $b_i$ for movies. The model can be improved once again.

$$Y_{u,i} = \mu + + b_i + b_u + \epsilon_{u,i}$$  

```{r user_RMSE, echo = FALSE}
users_avgs <- train_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

```{r user_RMSE table, echo = FALSE}
predicted_b_u <- test_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(users_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
plus_user_rmse <- RMSE(predicted_b_u, test_set$rating)
rmse_results <- rbind(rmse_results, 
                          data.frame(method = "Movie + User Effect Model", 
                                     RMSE = round(plus_user_rmse, 5)))
rmse_results %>% knitr::kable()
```


### Adding the genre factor

The variable of movie genre also was one of the factors in which the rating depended also had an effect on the average rating given to said movie and user. Here, $b_g$ will represent this effect.

$$Y_{u,i} = \mu + b_i + b_u + b_g + \epsilon_{u,i}$$  

```{r gender_RMSE, echo = FALSE}
genres_avgs <- train_set %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(movies_avgs, by = "movieId") %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))
```

```{r gender_RMSE table, echo = FALSE}
predicted_b_g <- test_set %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(genres_avgs, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  .$pred
plus_gender_rmse <- RMSE(predicted_b_g, test_set$rating)
rmse_results <- rbind(rmse_results, 
                      data.frame(method = "Movie + User + Genre Effect Model", 
                                 RMSE = round(plus_gender_rmse, 5)))
rmse_results %>% knitr::kable()
```


### Adding the release year factor

The date of review also had an effect on the average rating given to said movie and user. As demonstrated in the exploratory section, movies of a certain decade and years had on average. Here, $b_y$ represents this effect.

$$Y_{u,i} = \mu + b_i + b_u + b_g + b_y + \epsilon_{u,i}$$  

```{r year_RMSE, echo = FALSE}
year_avgs <- train_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(genres_avgs, by = "genres") %>%
  group_by(release_year) %>%
  summarize(b_y = mean(rating - mu - b_i - b_u - b_g))
```

```{r year_RMSE table, echo = FALSE}
predicted_b_y <- test_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(genres_avgs, by = "genres") %>%
  left_join(year_avgs, by = "release_year") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y) %>%
  .$pred
plus_year_rmse <- RMSE(predicted_b_y, test_set$rating)
rmse_results <- rbind(rmse_results, 
                      data.frame(method = "Movie + User + Genre + Release Year Effect Model", 
                                 RMSE = round(plus_year_rmse, 5)))
rmse_results %>% knitr::kable()
```


### Adding the rating date factor

The fifth and last bias for adjusting the model is the rating date of the movie $i$ by user $u$.
The date of review also had an effect on the average rating given to said movie and user. To adjust for the swift changes in rating by date, the rating week will by used to smooth these abrupt changes. Here, $b_r$ will represent this effect.

$$Y_{u,i} = \mu + b_i + b_u + b_g + b_y + b_r +\epsilon_{u,i}$$  

```{r date_RMSE, echo = FALSE}
date_avgs <- train_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(genres_avgs, by = "genres") %>%
  left_join(year_avgs, by = "release_year") %>%
  group_by(rating_week) %>%
  summarize(b_r = mean(rating - mu - b_i - b_u - b_g - b_y))
```

```{r date_RMSE table, echo = FALSE}
predicted_b_r <- test_set %>%
  left_join(movies_avgs, by = "movieId") %>%
  left_join(users_avgs, by = "userId") %>%
  left_join(genres_avgs, by = "genres") %>%
  left_join(year_avgs, by = "release_year") %>%
  left_join(date_avgs, by = "rating_week") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y + b_r) %>%
  .$pred
plus_date_rmse <- RMSE(predicted_b_r, test_set$rating)
rmse_results <- rbind(rmse_results, 
                          data.frame(method = "Movie + User + Genre + Release Year + Rating Date Effect Model", 
                                     RMSE = round(plus_date_rmse, 5)))
rmse_results %>% knitr::kable()
```

### Regularizing the model

Regularization, simply put, permits us to penalize large estimates that come from small sample sizes. The general idea is to add a penalty for the large values of $b$ to the sum of squares equations that we minimize. Since having many large $b$s make it harder to minimize the equation that we're trying to minimize.
This is done with the addition of a penalty term that gets larger when many $b$s are large to the residual sum of square.

$$\frac{1}{N} \sum_{u,i} \left( y_{u,i} - \mu - b_i - b_u - b_g - b_y -b_r \right)^2 + \lambda\left(\sum_ib_i^2 + \sum_ub_u^2 + \sum_gb_g^2 + \sum_yb_y^2 + \sum_rb_r^2 \right)$$ 

This function can be rewritten like this:


$$\hat{b}_r \left( \lambda \right) = \frac{1}{\lambda + n_r} \sum_{u = 1}^{n_r} \left(Y_{u,i} - \hat{\mu} - \hat{b_i} - \hat{b_u} - \hat{b_g} - \hat{b_y}\right)$$

When $n_r$ is very large which will give us a stable estimate, then lambda is effectively ignored because $n_r$ plus $lambda$ is about equal to $n_r$. However, when $n_i$ is small, then the estimate of $b_r$ is shrunken towards zero. The larger $lambda$ the more we shrink.


```{r regularization on the final model, echo = FALSE, message = FALSE}
#Cross validation lambdas parameters
lambdas <- seq(4.5, 5.5, 0.01)
#Regularised model, predict ratings and calculate RMSE for each value of lambda
rmses <- sapply(lambdas, function(l){

mu <- mean(train_set$rating)
#Movies
b_i <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n() + l))
#plus users
b_u <- train_set %>%
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n() + l))
#plus genres
b_g <- train_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_i - b_u - mu)/(n() + l))
#plus release year
b_y <- train_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  group_by(release_year) %>%
  summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/(n() + l))
#plus rating date
b_r <- train_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  left_join(b_y, by="release_year") %>%
  group_by(rating_week) %>%
  summarize(b_r = sum(rating - b_i - b_u - b_g - b_y - mu)/(n() + l))
#all together to prediction
predicted_ratings <- test_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  left_join(b_g, by="genres") %>%
  left_join(b_y, by="release_year") %>%
  left_join(b_r, by="rating_week") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y + b_r) %>%
  .$pred
return(RMSE(predicted_ratings, test_set$rating))
})
#Optimal lambda
lambda <- lambdas[which.min(rmses)]
#Minimum RMSE
regularised_rmse <- min(rmses)
qplot(lambdas, rmses) + 
geom_hline(yintercept=min(rmses), linetype='dashed', color = "red") +
annotate("text", x = lambda, y = min(rmses), label = lambda, vjust = -1, color = "red")
#Adding the regularised_rmse to the table
rmse_results <- rbind(rmse_results, 
                    data.frame(method = "Reguralized User + Movie + Genre + Release year + Rating Date Effect Model", 
                               RMSE = round(regularised_rmse, 5)))
rmse_results %>% knitr::kable()
```


## Testing out the final model

Finally, here is result of the final model on the movielens final_holdout_test set.

```{r regularization holdout on the final model, echo = FALSE, message = FALSE, warning = FALSE}
#making the proper mutations
final_holdout_test_m <- final_holdout_test %>%
  mutate(release_year = as.numeric(str_sub(title, -5, -2))) %>%
  mutate(rating_date = as.Date(as.POSIXct(timestamp, origin = "1970-01-01")), 
         rating_week = round_date(as_datetime(timestamp), unit = "week"), 
         season = time2season(rating_date, out.fmt = "seasons"))

#Cross validation lambdas parameters
lambdas <- seq(4.5, 5.5, 0.01)
#Regularised model, predict ratings and calculate RMSE for each value of lambda
rmses_final_model <- sapply(lambdas, function(l){
  
  mu <- mean(edx$rating)
  #Movies
  b_i <- edx_m %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n() + l))
  #plus users
  b_u <- edx_m %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n() + l))
  #plus genres
  b_g <- edx_m %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - b_u - mu)/(n() + l))
  #plus release year
  b_y <- edx_m %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(release_year) %>%
    summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/(n() + l))
  #plus rating date
  b_r <- edx_m %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_y, by="release_year") %>%
    group_by(rating_week) %>%
    summarize(b_r = sum(rating - b_i - b_u - b_g - b_y - mu)/(n() + l))
  #all together to prediction
  predicted_ratings <- final_holdout_test_m %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    left_join(b_y, by="release_year") %>%
    left_join(b_r, by="rating_week") %>%
    mutate(pred = mu + b_i + b_u + b_g + b_y + b_r) %>%
    .$pred
  return(RMSE(predicted_ratings, final_holdout_test_m$rating))
})
#Optimal lambda
lambda_final_model <- lambdas[which.min(rmses_final_model)]
#Minimum RMSE
regularised_rmse_final <- min(rmses_final_model)
#Adding the regularised_final_houldout
rmse_results_final <- data.frame(method = "Project objective", RMSE = "0.86490")
rmse_results_final <- rmse_results_final %>% rbind(c("Final Model Validation", 
                                         round(regularised_rmse_final, 5)))
rmse_results_final %>% knitr::kable()
```



# Conclusion

The aim of this project was to create a recommendation system that could predict a RMSP of less than 0.86490 using the MovieLens dataset. To achieve it, the approach involved adjusting for biases caused by movie, user, genre, release year and review date (based on a week round). The next step in the approach was regularizing these biases to constrain the variability of effect sizes. The final outcome in the test set was a RMSE of 0.86384. When the algorithm was then implemented into the final_holdout_test set a RMSE of 0.86405 was produced.


# References

* http://rafalab.dfci.harvard.edu/dsbook/large-datasets.html
* https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a
